#1. Se procede con la carga de los datos
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

excel_file='DatosRiesgoCredito.xlsx'

BDCA=pd.read_excel(excel_file,sheet_name=0)
BDCN=pd.read_excel(excel_file,sheet_name=1)

#Ahora se procede a la selección de los individuos que poseen puntaje
FPA=np.where(~np.isnan(BDCA['Puntaje']))
FPN=np.where(~np.isnan(BDCN['Puntaje']))

BDA=np.array(BDCA.iloc[FPA])
BDN=np.array(BDCN.iloc[FPN])

XP=np.vstack((BDA,BDN))
XP1=np.array([XP[:,1],XP[:,4],XP[:,5],XP[:,6],XP[:,9],XP[:,10],XP[:,12],XP[:,13],XP[:,14],XP[:,21]])
XP2=np.array(XP1.astype(float))
XP2=np.transpose(XP2)
print(XP2)

#Para normalizar los datos de un array por columnas
from sklearn.preprocessing import normalize 
import seaborn as sns

X = normalize(XP2, axis=0, norm='max') 
print(X)

#2. Se procede con la descripción de los datos. 
import seaborn as sb
df.describe()  
df.drop(['Estrato'],1).hist()
plt.show()

sb.pairplot(df.dropna(),hue='Estrato',size=4,vars=["Ingresos","Egresos"],kind='scatter')
sb.pairplot(df.dropna(),size=4,vars=["Ingresos","Egresos"],kind='scatter')
df.corr()

#3. Se procede a determinar el número de clusters para el modelo
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min

NO = range(1, 20)
kmeans = [KMeans(n_clusters=i) for i in NO]
kmeans
score = [kmeans[i].fit(X).score(X) for i in range(len(kmeans))]
score
plt.plot(NO,score)
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.show()

#4. Se ejecuta el algoritmo k-means
NO=5
XD=X[:,0:9]
XD.shape
yd=X[:,9]
print(yd[0,])

kmeans=KMeans(n_clusters=NO).fit(XD)
centroids=kmeans.cluster_centers_
df2=pd.DataFrame(centroids,columns=["Edad","Hijos","PersCargo","Estrato","Ingresos","Egresos","Monto","Plazo","Cuota"])
df2.head()
XC=np.array(centroids)
D=np.zeros((5,1))

for i in range(5):
  D[i,]=np.sum(np.power(XC-XC[i,:],2))/4;

print(D)
XCA=XC

#5. Se crea la red neuronal de base radial 
from random import random

NE=9; ND=217; NO=5; NIT=1000; alfa=0.01;
C=np.random.random((NO,1))
h=np.zeros((NO,1))
ys=np.zeros((ND,1)); ek=np.zeros((ND,1))
ek2=np.zeros((NIT,1))
   
#Configuración del modelo adaptación y aprendizaje 
for i1 in range(NIT):
  ek2[i1,]=0

  #Este ciclo repetitivo solamente recorre la tabla una sola vez
  for k in range(ND):
    h=np.exp(-0.5*np.sum(np.power((XC-XD[k,:])/D,2),axis=1))  #Paso 1: Proceso Feedforward
    ys[k,]=np.dot(np.transpose(C),h)
    ek[k,]=yd[k,]-ys[k,]
    ek2[i1,]=ek2[i1,]+np.power((ek[k,]),2)
    C=C+alfa*((alfa*ek[k,])*h.reshape(5,1))
    XC=XC+(alfa*ek[k,])*np.dot((C.reshape(5,1)*h.reshape(1,5)),(-(XC-XD[k,])/np.power(D,2)))
    D=D+(alfa*ek[k,])*np.sum(np.power((XC-XD[k,:])/np.power(D,3),2),axis=1).shape
    	        	
plt.figure()
plt.plot(yd)
plt.plot(ys)
plt.show

plt.figure()
plt.plot(ek2)
plt.show

a=np.dot(C.reshape(1,NO),XC)
df2=pd.DataFrame(a,columns=["Edad","Hijos","PersCargo","Estrato","Ingresos","Egresos","Monto","Plazo","Cuota"])
df2.head()

XM=np.column_stack((yd,ys))
df1=pd.DataFrame(XM,columns=['yd','ys'])
print(df1.corr())

np.corrcoef(XCA,XC)



df=pd.DataFrame(X,columns=["Edad","Hijos","PersCargo","Estrato","Ingresos","Egresos","Monto","Plazo","Cuota","Puntaje"])
df.head()